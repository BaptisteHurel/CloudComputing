{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notebook Data Pipelines et Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 classes:**\n",
    "- Data Handler\n",
    "- Feature Recipe\n",
    "- Future Extactor\n",
    "- Model Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "- https://storage.googleapis.com/h3-data/listings_final.csv\n",
    "- https://storage.googleapis.com/h3-data/price_availability.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer vos librairies  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Toutes les Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataHandler\n",
    "- Feature Recipe\n",
    "- Future Extactor\n",
    "- Model Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Classe DataHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 attributs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    \"\"\"\n",
    "        Getting data from bucket\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Initialising the 3 datasets :\n",
    "            entry 1\n",
    "            entry 2\n",
    "            result\n",
    "        \"\"\"\n",
    "        print(\"DataHandlerintialisation\")\n",
    "        self.df_lf = None\n",
    "        self.df_pa = None\n",
    "        self.df_res = None\n",
    "        print(\"intialisation done\")\n",
    "\n",
    "    def get_data(self):\n",
    "        print(\"loading data from bucket\")\n",
    "        self.df_lf = pd.read_csv(\"https://storage.googleapis.com/h3-data/listings_final.csv\",sep=';')\n",
    "        self.df_pa = pd.read_csv(\"https://storage.googleapis.com/h3-data/price_availability.csv\",sep=';')\n",
    "        print(\"data loaded from bucket\")\n",
    "\n",
    "    def group_data(self):\n",
    "        print(\"merging data\")\n",
    "        self.df_res = pd.merge(self.df_lf,self.df_pa.groupby('listing_id')['local_price'].mean('local_price'),how='inner', on='listing_id')\n",
    "        print(\"size of the merged data : {} lines, {} columns\".format(self.df_res.shape[0],self.df_res.shape[1]))\n",
    "\n",
    "    def get_process_data(self):\n",
    "        self.get_data()\n",
    "        self.group_data()\n",
    "        print(\"end of processing\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandlerintialisation\n",
      "intialisation done\n"
     ]
    }
   ],
   "source": [
    "d1 = DataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from bucket\n"
     ]
    }
   ],
   "source": [
    "d1.get_process_data()\n",
    "%time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.df_merge.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.df_merge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Classe FutureRecipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureRecipe:\n",
    "    \n",
    "    def __init__(self, data: pds.DataFrame):\n",
    "        print(\"FeatureRecipe starts...\")\n",
    "        self.df = data\n",
    "        self.cate = []\n",
    "        self.floa = []\n",
    "        self.intt = []\n",
    "        print(\"End of FeatureRecipe initialisation\\n\")\n",
    "    \n",
    "    def separate_variable_types(self) -> None:\n",
    "        print(\"Separate variable types starts...\")\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtypes == int:\n",
    "                self.intt.append(self.df[col])\n",
    "            elif self.df[col].dtypes == float:\n",
    "                self.floa.append(self.df[col])\n",
    "            else:\n",
    "                self.cate.append(self.df[col])\n",
    "        print(\"Separate variable types end...\")\n",
    "        print (\"Dataset number of columns : {} \\nnumber of discreet values : {} \\nnumber of continuous values : {} \\nnumber of others : {} \\ntotal size : {}\\n\".format(len(self.df.columns),\n",
    "        len(self.intt),len(self.floa),len(self.cate),len(self.intt)+len(self.floa)+len(self.cate) ))\n",
    "        \n",
    "    def drop_uselessf(self):\n",
    "        print(\"Drop useless feature start...\")\n",
    "        \n",
    "        if \"Unnamed: 0\" in self.df.columns:\n",
    "            self.df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "            \n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isna().sum() == len(self.df[col]):\n",
    "                self.df.drop([col], axis=1, inplace=True)\n",
    "                \n",
    "        print(\"Drop useless feature end...\")\n",
    "        print(\"Number columns remaining {}\\n\".format(len(self.df.columns)))\n",
    "        \n",
    "    def deal_duplicate(self):\n",
    "        print(\"Deal duplicate start...\")\n",
    "        dropped_duplicates = self.df.drop_duplicates(inplace=True)\n",
    "        print(\"Dropped duplicates : {}\".format(dropped_duplicates))\n",
    "        print(\"Deal duplicate end...\")\n",
    "    \n",
    "    def drop_nanp(self, thresold: float):\n",
    "        \"\"\"\n",
    "        Drop NaN columns according to a thresold\n",
    "        \"\"\"\n",
    "        def deal_nanp(df:pd.DataFrame, thresold: float):\n",
    "            bf=[]\n",
    "            for c in self.data.columns.to_list():\n",
    "                if self.data[c].isna().sum()/self.data.shape[0] > thresold:\n",
    "                    bf.append(c)\n",
    "            logging.debug(\"{} feature have more than {} NaN \".format(len(bf), thresold))\n",
    "            logging.debug('\\n\\n - - - features - - -  \\n {}'.format(bf))\n",
    "            return bf \n",
    "        self.data = self.data.drop(deal_nanp(self.data, thresold), axis=1)\n",
    "        logging.debug('Some NaN features droped according to {} thresold'.format(thresold))\n",
    "    \n",
    "    def get_duplicates(self):\n",
    "        print(\"Get duplicates\")\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            for row in range(self.df.shape[0]):\n",
    "                        \n",
    "    def deal_dtime(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_data(self, threshold: float):\n",
    "        self.drop_uselessf()\n",
    "        self.separate_variable_types()\n",
    "        self.deal_duplicate()\n",
    "        self.drop_nanp(threshold)\n",
    "        self.deal_dtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fterecipe = FeatureRecipe(d1.df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fterecipe.prepare_data(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Classe FeatureExtractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skn\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature Extractor class\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, flist: list):\n",
    "        \"\"\"\n",
    "            Input : pandas.DataFrame, feature list to drop\n",
    "            Output : X_train, X_test, y_train, y_test according to sklearn.model_selection.train_test_split\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None,None,None,None\n",
    "        self.df = data\n",
    "        self.flist = flist\n",
    "        \n",
    "    def splitting(size:float,rng:int,X pd.Series):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test train_test_split(X, y, size, rng)\n",
    "        \n",
    "    def train():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.equals(otherCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Classe ModelBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    \"\"\"\n",
    "        Class for train and print results of ml model \n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str = None, save: bool = None):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        pass\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        pass\n",
    "    def save_model(self, path:str):\n",
    "        #with the format : 'model_{}_{}'.format(date)\n",
    "        pass\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            #load model\n",
    "            pass\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataManager(d:DataHandler=None, fr: FeatureRecipe=None, fe:FeatureExtractor=None):\n",
    "    \"\"\"\n",
    "        Fonction qui lie les 4 classes de la pipeline et qui return FeatureExtractor.split(0.1)\n",
    "    \"\"\"\n",
    "    pass\n",
    "#on appelera la fonction DataManager() de la facon suivante : \n",
    "X_train, X_test, y_train, y_test = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ModelBuilder() \n",
    "m.train(X_train, y_train)\n",
    "m.print_accuracy()\n",
    "m.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
